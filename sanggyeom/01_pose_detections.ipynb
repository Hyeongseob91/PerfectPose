{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6f7a47659d4b98aed3fe8297f48e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/172M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\lang_311\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--PekingU--rtdetr_r50vd_coco_o365. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51f0d3cf24c45e8a02133d2c359e501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\lang_311\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--usyd-community--vitpose-base-simple. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641ce0c7928c4862b97915b7b6113a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16380a2680ca4553b254bdbfe11b3807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person #0\n",
      " - Nose: x=428.72, y=170.61, score=0.92\n",
      " - L_Eye: x=429.48, y=167.83, score=0.90\n",
      " - R_Eye: x=428.74, y=168.17, score=0.79\n",
      " - L_Ear: x=433.88, y=167.35, score=0.94\n",
      " - R_Ear: x=441.09, y=166.86, score=0.90\n",
      " - L_Shoulder: x=440.02, y=177.15, score=0.93\n",
      " - R_Shoulder: x=446.28, y=178.39, score=0.74\n",
      " - L_Elbow: x=436.88, y=197.90, score=0.92\n",
      " - R_Elbow: x=433.36, y=201.21, score=0.54\n",
      " - L_Wrist: x=431.45, y=218.66, score=0.88\n",
      " - R_Wrist: x=420.10, y=212.79, score=0.96\n",
      " - L_Hip: x=444.81, y=224.15, score=0.81\n",
      " - R_Hip: x=452.34, y=223.91, score=0.82\n",
      " - L_Knee: x=442.23, y=256.02, score=0.83\n",
      " - R_Knee: x=451.12, y=255.19, score=0.82\n",
      " - L_Ankle: x=443.18, y=288.16, score=0.60\n",
      " - R_Ankle: x=456.03, y=285.75, score=0.82\n",
      "Person #1\n",
      " - Nose: x=398.12, y=181.71, score=0.87\n",
      " - L_Eye: x=398.45, y=179.73, score=0.82\n",
      " - R_Eye: x=396.07, y=179.45, score=0.90\n",
      " - R_Ear: x=388.85, y=180.22, score=0.88\n",
      " - L_Shoulder: x=397.24, y=194.16, score=0.76\n",
      " - R_Shoulder: x=384.60, y=190.74, score=0.64\n",
      " - L_Wrist: x=402.25, y=207.04, score=0.33\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    RTDetrForObjectDetection,\n",
    "    VitPoseForPoseEstimation,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Stage 1. Detect humans on the image\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# You can choose detector by your choice\n",
    "person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n",
    "person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n",
    "\n",
    "inputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = person_model(**inputs)\n",
    "\n",
    "results = person_image_processor.post_process_object_detection(\n",
    "    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n",
    ")\n",
    "result = results[0]  # take first image results\n",
    "\n",
    "# Human label refers 0 index in COCO dataset\n",
    "person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n",
    "person_boxes = person_boxes.cpu().numpy()\n",
    "\n",
    "# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\n",
    "person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n",
    "person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Stage 2. Detect keypoints for each person found\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n",
    "model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)\n",
    "\n",
    "inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes], threshold=0.3)\n",
    "image_pose_result = pose_results[0]  # results for first image\n",
    "\n",
    "for i, person_pose in enumerate(image_pose_result):\n",
    "    print(f\"Person #{i}\")\n",
    "    for keypoint, label, score in zip(\n",
    "        person_pose[\"keypoints\"], person_pose[\"labels\"], person_pose[\"scores\"]\n",
    "    ):\n",
    "        keypoint_name = model.config.id2label[label.item()]\n",
    "        x, y = keypoint\n",
    "        print(f\" - {keypoint_name}: x={x.item():.2f}, y={y.item():.2f}, score={score.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m image_pil = Image.fromarray(image)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 📌 4-2. ViTPose 모델 입력 전처리 (boxes=[] 추가)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m inputs_pose = \u001b[43mpose_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_pil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_height\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 전체 이미지를 대상으로 포즈 감지\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     53\u001b[39m     outputs_pose = pose_model(**inputs_pose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\lang_311\\Lib\\site-packages\\transformers\\image_processing_utils.py:42\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\lang_311\\Lib\\site-packages\\transformers\\models\\vitpose\\image_processing_vitpose.py:518\u001b[39m, in \u001b[36mVitPoseImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, boxes, do_affine_transform, size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image, image_boxes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, boxes):\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m image_boxes:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         center, scale = \u001b[43mbox_to_center_and_scale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_width\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwidth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_height\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnormalize_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m         transformed_image = \u001b[38;5;28mself\u001b[39m.affine_transform(\n\u001b[32m    525\u001b[39m             image, center, scale, rotation=\u001b[32m0\u001b[39m, size=size, input_data_format=input_data_format\n\u001b[32m    526\u001b[39m         )\n\u001b[32m    527\u001b[39m         new_images.append(transformed_image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\lang_311\\Lib\\site-packages\\transformers\\models\\vitpose\\image_processing_vitpose.py:85\u001b[39m, in \u001b[36mbox_to_center_and_scale\u001b[39m\u001b[34m(box, image_width, image_height, normalize_factor, padding_factor)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbox_to_center_and_scale\u001b[39m(\n\u001b[32m     57\u001b[39m     box: Union[Tuple, List, np.ndarray],\n\u001b[32m     58\u001b[39m     image_width: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     padding_factor: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1.25\u001b[39m,\n\u001b[32m     62\u001b[39m ):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m    Encodes a bounding box in COCO format into (center, scale).\u001b[39;00m\n\u001b[32m     65\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m        - `np.ndarray` [float32](2,): Scale of the bbox width & height.\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     top_left_x, top_left_y, width, height = \u001b[43mbox\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     86\u001b[39m     aspect_ratio = image_width / image_height\n\u001b[32m     87\u001b[39m     center = np.array([top_left_x + width * \u001b[32m0.5\u001b[39m, top_left_y + height * \u001b[32m0.5\u001b[39m], dtype=np.float32)\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, VitPoseForPoseEstimation\n",
    "\n",
    "# 📌 1. GPU 설정 (사용 가능하면 CUDA, 아니면 CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 📌 2. ViTPose 모델 및 프로세서 로드\n",
    "model_name = \"usyd-community/vitpose-base-simple\"\n",
    "pose_processor = AutoProcessor.from_pretrained(model_name)\n",
    "pose_model = VitPoseForPoseEstimation.from_pretrained(model_name).to(device)\n",
    "pose_model.eval()\n",
    "\n",
    "# 📌 3. 동영상 로드\n",
    "video_path = \"data/sample1_360.mp4\"  # 입력 동영상 파일\n",
    "output_path = \"data/sample1_360_out.mp4\"  # 결과 저장 파일\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# COCO 데이터셋 기준의 관절 연결 정보\n",
    "skeleton = [\n",
    "    (5, 7), (7, 9), (6, 8), (8, 10),  # 팔 (오른쪽, 왼쪽)\n",
    "    (11, 13), (13, 15), (12, 14), (14, 16),  # 다리 (오른쪽, 왼쪽)\n",
    "    (5, 6), (11, 12), (5, 11), (6, 12)  # 몸통 연결\n",
    "]\n",
    "\n",
    "# 📌 4. 동영상 프레임별 처리 루프\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 📌 4-1. OpenCV 프레임을 PIL 이미지로 변환\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Image.fromarray(image)\n",
    "\n",
    "    # 📌 4-2. ViTPose 모델 입력 전처리 (boxes=[] 추가)\n",
    "    inputs_pose = pose_processor(\n",
    "        images=image_pil, \n",
    "        boxes=[[0, 0, frame_width, frame_height]],  # 전체 이미지를 대상으로 포즈 감지\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_pose = pose_model(**inputs_pose)\n",
    "\n",
    "    # 포즈 데이터 후처리\n",
    "    pose_results = pose_processor.post_process_pose_estimation(outputs_pose, threshold=0.3)\n",
    "    image_pose_result = pose_results[0]  # 첫 번째 이미지의 결과\n",
    "\n",
    "    # 📌 4-3. 포즈 시각화\n",
    "    for person_pose in image_pose_result:\n",
    "        for keypoint, label, score in zip(person_pose[\"keypoints\"], person_pose[\"labels\"], person_pose[\"scores\"]):\n",
    "            if score.item() > 0.5:  # 신뢰도 50% 이상인 경우만 시각화\n",
    "                x, y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "                cv2.circle(frame, (x, y), 4, (0, 255, 0), -1)  # 관절 점 시각화\n",
    "\n",
    "    # 📌 4-4. 관절 연결선 (스켈레톤) 시각화\n",
    "    for pt1, pt2 in skeleton:\n",
    "        if len(image_pose_result) > 0:\n",
    "            keypoints = image_pose_result[0][\"keypoints\"]\n",
    "            if keypoints[pt1][2] > 0.5 and keypoints[pt2][2] > 0.5:\n",
    "                x1, y1 = int(keypoints[pt1][0]), int(keypoints[pt1][1])\n",
    "                x2, y2 = int(keypoints[pt2][0]), int(keypoints[pt2][1])\n",
    "                cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # 관절 연결선 시각화\n",
    "\n",
    "    # 📌 4-5. 결과 프레임 저장\n",
    "    out.write(frame)\n",
    "\n",
    "    # 📌 4-6. 화면 출력 (실시간 보기, 'q' 키로 종료 가능)\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 📌 5. 리소스 해제\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
